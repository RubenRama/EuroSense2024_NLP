---
title: "Natural Language Processing for sensory and consumer scientists. A tidy introduction in R"
author: "Ruben Rama"
date: 09/08/2024
date-format: "D MMM, YYYY"
format: 
  revealjs:
    incremental: true
    slide-number: true
    transition: slide
    # transition-speed: fast
    preview-links: true
    embed-resources: true
    logo: SYM_Logo.png
    css: eurosense.css
    footer: EuroSense 2024
    echo: true
    theme: default
    height: 900
    width: 1600
bibliography: references.bib
execute:
  cache: true
from: markdown+emoji
---


# Housekeeping


## Why Are We Here? 

:::: {.columns}

::: {.column width="50%"}

![](img/EuroSense_2024.jpg){width=600 fig-align="center"}

:::

::: {.column width="50%"}

![](img/sensoc2.PNG){.absolute top=75 left=1500}
[![](img/sensometrics_society.png){width=600 fig-align="right"}](https://sensometric.org/)

:::

::::

:::: {.columns}

::: {.column width="20%"}

:::

::: {.column width="60%"}

::: {style="font-size: 2em; text-align: center"}

EUROSENSE 2024

:::

::: {style="font-size: 1.5em; text-align: center"}

A Sense of Global Culture

:::

::: {style="font-size: 1.2em; text-align: center"}

11th Conference on Sensory and Consumer Research

8-11 September 2024

Dublin, Ireland

:::

:::

::: {.column width="20%"}

:::

::::


## Intro {auto-animate=true}

::: {.r-fit-text}

Hello! :wave:

:::


## Intro {auto-animate=true}

::: {style="font-size: 4em; text-align: center"}

Hello! :wave:

:::

:::: {.columns}

::: {.column width="50%"}

::: {style="font-size: 1.5em; text-align: center"}

::: {.fragment}

Ruben Rama

:::

:::

::: {style="text-align: center"}

::: {.fragment}

Global Sensory and Consumer Insights Data and Knowledge Manager

:::

:::

:::

::: {.column width="50%"}

::: {.fragment}

[![](SYM_Logo.png){fig-align="center"}](https://www.symrise.com/)

:::

:::

::::


## Where to find the workshop material?

:::: {.columns}

::: {.column width="25%"}

:::

::: {.column width="50%"}

![](img/github.png)
:::

::: {.column width="25%"}

:::

::::

::: {style="font-size: 1.5em; text-align: center"}

[https://github.com/RubenRama/EuroSense2024_NLP](https://github.com/RubenRama/EuroSense2024_NLP)

:::


# Preface


## What Are We Going to Do?

- Step-by-step guide to help sensory and consumer scientists start their journey into Natural Language Processing (NLP) analysis.


- Natural Language Processing (NLP) is a field of Artificial Intelligence that makes human language intelligible to machines.


- NLP studies the rules and structure of language, and create *intelligent* systems capable of:

  - understanding, 
  - analyzing, and 
  - extracting meaning from text. 


## What Are We Going to Do?

::: {style="font-size: 2em"}

- *Part One*: Text Mining and Exploratory Analysis


- *Part Two*: Tidy Sentiment Analysis in R


- *Part Three*: Topic Modelling

:::


## Tools :wrench: - [R](https://cran.r-project.org/) & [RStudio](https://posit.co/downloads/) {auto-animate=true}

:::: {.columns}

::: {.column width="50%" }

![](img/R.png){width="400" height="400" fig-align="center"}

:::

::: {.column width="50%"}

![](img/RStudio.png){width="700" height="400" fig-align="center"}

:::

::::


## Tools :wrench: - [R](https://cran.r-project.org/) & [RStudio](https://posit.co/downloads/) {auto-animate=true}

:::: {.columns}

::: {.column width="50%" }

![](img/R.png){width="300" height="250" fig-align="center"}

::: {style="text-align: center"}

[R](https://cran.r-project.org/) is the underlying statistical computing environment, but using [R](https://cran.r-project.org/) alone is no fun. 

:::

:::

::: {.column width="50%"}

![](img/RStudio.png){width="500" height="250" fig-align="center"}

::: {style="text-align: center"}

[RStudio](https://posit.co/downloads/) is a graphical integrated development environment (IDE) that makes using [R](https://cran.r-project.org/) much easier and more interactive.

:::

:::

::::

## Tools :wrench: - [R](https://cran.r-project.org/) & [RStudio](https://posit.co/downloads/) {auto-animate=true}

:::: {.columns}

::: {.column width="50%" }

![](img/R.png){width="300" height="250" fig-align="center"}

:::

::: {.column width="50%"}

![](img/RStudio.png){width="500" height="250" fig-align="center"}

:::

::::

. . .

[R](https://cran.r-project.org/) and [RStudio](https://posit.co/downloads/) are separate downloads and installations.

. . .

You need to install [R](https://cran.r-project.org/) before you install [RStudio](https://posit.co/downloads/). 

. . .

Once installed, because [RStudio](https://posit.co/downloads/) is an IDE, [RStudio](https://posit.co/downloads/) will run [R](https://cran.r-project.org/) in the background. You do not need to run it separately.


## Tools :mag: - [`tidyverse`](https://www.tidyverse.org/)

![](img/dplyr.png){.absolute top=150 left=50 width="100" height="100"}

![](img/ggplot2.png){.absolute top=230 left=0 width="100" height="100"}

![](img/forcats.png){.absolute top=230 left=105 width="100" height="100"}

![](img/tibble.png){.absolute top=230 left=210 width="100" height="100"}

![](img/readr.png){.absolute top=310 left=50 width="100" height="100"}

![](img/stringr.png){.absolute top=310 left=155 width="100" height="100"}

![](img/tidyr.png){.absolute top=310 left=265 width="100" height="100"}

![](img/purrr.png){.absolute top=390 left=210 width="100" height="100"}


:::: {.columns}

::: {.column width="40%"}

:::

::: {.column width="60%"}

::: {.fragment}

We will be using `tidy` principles.

:::

::: {.fragment}

The [`tidyverse`](https://www.tidyverse.org/) is an opinionated collection of R packages designed for data science.

:::

::: {.fragment}

All packages share an underlying design philosophy, grammar, and data structures.

:::

:::

::::


## Tools :mag: - [`tidyverse`](https://www.tidyverse.org/) {auto-animate=true}

![](img/dplyr.png){.absolute top=150 left=50 width="100" height="100"}

![](img/ggplot2.png){.absolute top=230 left=0 width="100" height="100"}

![](img/forcats.png){.absolute top=230 left=105 width="100" height="100"}

![](img/tibble.png){.absolute top=230 left=210 width="100" height="100"}

![](img/readr.png){.absolute top=310 left=50 width="100" height="100"}

![](img/stringr.png){.absolute top=310 left=155 width="100" height="100"}

![](img/tidyr.png){.absolute top=310 left=265 width="100" height="100"}

![](img/purrr.png){.absolute top=390 left=210 width="100" height="100"}

:::: {.columns}

::: {.column width="40%"}

:::

::: {.column width="60%"}


::: {.fragment}

`tidy` data has a specific structure:

:::

- Each variable is a column.
- Each observation is a row.
- Each type of observational unit is a table

::: {.fragment}

![](img/tidy_data.png)^[https://r4ds.hadley.nz/data-tidy.html#fig-tidy-structure]

:::

:::

::::


##  Tools :mag: - [`tidyverse`](https://www.tidyverse.org/) {auto-animate=true}

:::: {.columns}

::: {.column width="40%"}

![](img/tidyverse.png){width="400" height="400" fig-align="center"}

::: 

::: {.column width="60%"}

::: {.fragment}

We can install the complete [`tidyverse`](https://www.tidyverse.org/) with:

```r
install.packages("tidyverse")
```

:::

::: {.fragment}

Once installed, we can load it with:

```{r}
library(tidyverse)
```

:::

:::

::::


## Tools :book: - [`tidyverse`](https://www.tidyverse.org/)

:::: {.columns}

::: {.column width="50%"}

![](img/R4DS.jpg){width="400" height="500" fig-align="center"}

::: {style="text-align: center"}

[R for Data Science [@r_data_science]](https://r4ds.hadley.nz/)

:::

:::

::: {.column width="50%"}

![](img/tmwr.png){width="400" height="500" fig-align="center"}

::: {style="text-align: center"}

[Text Mining with R [@r_text_mining]](https://www.tidytextmining.com/)

:::

:::

::::


## Tools - Basic Piping {auto-animate="true"}

:::: {.columns}

::: {.column width="50%"}

::: {style="font-size: 9em; text-align: center"}

`%>%`

:::

:::

::: {.column width="50%"}

::: {style="font-size: 9em; text-align: center"}

`|>`

:::

:::

::::

:::: {.columns}

::: {.column width="50%"}

::: {style="text-align: center"}

::: {.fragment}

![](img/magrittr.png){width="100" height="100" fig-align="center"}

[`magrittr`](https://magrittr.tidyverse.org/index.html) package

:::

:::

:::

::: {.column width="50%"}

::: {style="text-align: center"}

::: {.fragment}

![](img/R.png){width="125" height="100" fig-align="center"}

[R](https://cran.r-project.org/) Native (> 4.1)

:::

:::

:::

::::


## Tools - Basic Piping {auto-animate="true"}

:::: {.columns}

::: {.column width="40%"}

::: {style="font-size: 9em; text-align: center"}

`|>`

:::

:::

::: {.column width="60%"}

::: {.fragment}

It allows us to link a sequence of analysis steps.

:::

::: {.fragment}

```r
function(data, argument(s))

# is equivalent to

data |> 
  funtion(argument(s))

```

:::

::: {.fragment}

The pipe operator 

- takes the thing that is on the left, and 
- places it on the first argument of the function that is on the right!

:::

:::

::::


## 

::: {style="font-size: 3em; text-align: center"}

Prepare your Questions!

:checkered_flag:

:::


# Part One: Text Mining and Exploratory Analysis

A Friendly Place


## Text Mining and Data Analysis

Data can be organized into three categories:

  - **structured** data: predefined and formatted to a tabular format (e.g., an Excel spreadsheet).
  - **semi-structured** data: blend between structured and unstructured (e.g., JSON files).
  - **unstructured** data: data with no predefined format (e.g., an email).


## Text Mining and Data Analysis

Text mining or text analysis is the process of *exploring* and *analyzing* **unstructured** or **semi-structured** text data to identify:

- key concepts,
- patterns,
- relationships or,
- any other attributes of the text.


## Text Mining and Data Analysis

From a sensory and consumer perspective, text data can come from lots of different sources:

- panel/consumer comments,
- product review,
- interview transcripts,
- MROCS or online discussions,
- digitized text,
- tweets, blogs, social media,
- etc. 


## Process of Text Mining

A simplistic explanation of a typical text mining can include the following steps:

![](img/simple_text_mining.png)

1. We gather the data, either by creating it or selecting existing datasets.
2. We preprocess or clean the text to get it ready for analysis.
3. We perform text mining or analysis, such as sentiment analysis, topic modelling, etc.
4. We communicate the findings from the text mining.


## Importing Review Data

Original data was sourced from [Kaggle](https://www.kaggle.com/), from the [Amazon Alexa Reviews](https://www.kaggle.com/datasets/sid321axn/amazon-alexa-reviews) dataset.

A copy of the data file is available in my github repository.

:::: {.columns}

::: {.column width="25%"}

:::

::: {.column width="50%"}

![](img/github.png)
:::

::: {.column width="25%"}

:::

::::

::: {style="font-size: 1.5em; text-align: center"}

[https://github.com/RubenRama/EuroSense2024_NLP](https://github.com/RubenRama/EuroSense2024_NLP)

:::


## Importing Review Data

We can use the [`read_csv()`](https://readr.tidyverse.org/reference/read_delim.html) function from [`readr`](https://readr.tidyverse.org/) (part of the [`tidyverse`](https://www.tidyverse.org/)) to load the data:

```{r}
review_data <- read_csv("data/amazon_alexa.csv")
```


## Importing Review Data

We can also use the [`file.choose()`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/file.choose) function. 

That will bring up a file explorer window that will allow us to interactively choose the required file:

```r
review_data <- read_csv(file.choose())
```


## Let's Explore the Data

```{r}
#| output-location: fragment
review_data
```


## Let's Explore the Data

We have **3150** reviews. 

We may want to remove any duplicated reviews by using the [`distinct()`](https://dplyr.tidyverse.org/reference/distinct.html) function from [`dplyr`](https://dplyr.tidyverse.org/index.html):

:::: {.columns}

::: {.column width="50%"}

::: {.fragment}

Traditional approach:

````r
distinct(review_data)
````

:::

:::

::: {.column width="50%"}

::: {.fragment}

With `|>`

````r
review_data |> 
  distinct()
````

:::

:::

::::


## Let's Explore the Data {auto-animate=true}

We have **3150** reviews. 

We may want to remove any duplicated reviews by using the [`distinct()`](https://dplyr.tidyverse.org/reference/distinct.html) function from [`dplyr`](https://dplyr.tidyverse.org/index.html):

```{r}
#| code-line-numbers: "|1,2"
review_data <- review_data |> 
  distinct()
```


## Let's Explore the Data {auto-animate=true}

We have **3150** reviews. 

We may want to remove any duplicated reviews by using the [`distinct()`](https://dplyr.tidyverse.org/reference/distinct.html) function from [`dplyr`](https://dplyr.tidyverse.org/index.html):

```{r}
#| output-location: fragment
review_data <- review_data |> 
  distinct()

review_data
```


## Let's Explore the Data

:::: {.columns}

::: {.column width="40%"}

![](img/dplyr-summarise.png){width=400 height=300 fig-align="center"}

:::

::: {.column width="60%"}

Briefly, let's just focus on one product.

We use [`filter()`](https://dplyr.tidyverse.org/reference/filter.html) and [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html) (or [`summarize()`](https://dplyr.tidyverse.org/reference/summarise.html)) functions (from [`dplyr`](https://dplyr.tidyverse.org/index.html)):

:::

::::

:::: {.columns}

::: {.column with="50%"}

::: {.fragment}

Traditional approach:

```{r}
#| code-line-numbers: "|1,2|4,5,6|8"
#| output-location: fragment
df <- filter(review_data,
             product == "Charcoal Fabric")

df <- aggregate(df$stars, 
                by = list(df$product),
                FUN = mean)

df
```

:::

:::

::: {.column width="50%"}

::: {.fragment}

with `|>`:

```{r}
#| code-line-numbers: "|1|2|3"
#| output-location: fragment
review_data |>
  filter(product == "Charcoal Fabric") |>
  summarise(stars_mean = mean(stars))
```

::: 

:::

::::


## Let's Explore the Data

:::: {.columns}

::: {.column width="40%"}

![](img/dplyr-group_by.png){width=400 height=300 fig-align="center"}

:::

::: {.column width="60%"}

We may want to group by product and then obtain a summary of the star rating.

We can use [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html) and [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html) (also from [`dplyr`](https://dplyr.tidyverse.org/index.html)):

:::

::::

```{r}
#| code-line-numbers: "|2|3"
#| output-location: column-fragment
review_data |>
  group_by(product) |>
  summarise(stars_mean = mean(stars))
```


## Let's Explore the Data

We can [`arrange()`](https://dplyr.tidyverse.org/reference/arrange.html) the results to show in descending order (yes! also [`dplyr`](https://dplyr.tidyverse.org/index.html)):

```{r}
#| code-line-numbers: "|4"
#| output-location: column-fragment
review_data |>
  group_by(product) |>
  summarize(stars_mean = mean(stars)) |>
  arrange(desc(stars_mean))
```


## Let's Explore the Data

But we cannot summarise unstructured or categorical data!

```{r}
#| code-line-numbers: "|2|3"
#| output-location: column-fragment
review_data |>
  group_by(product) |>
  summarize(review_mean = mean(review))
```


## Counting Categorical Data

If what we want is to understand the number of reviews per product, we can summarize with [`n()`](https://dplyr.tidyverse.org/reference/context.html) after grouping by product.

```{r}
#| code-line-numbers: "|2"
#| output-location: column-fragment
review_data |>
  group_by(product) |>
  summarize(number_rows = n())
```


## Counting Categorical Data

Alternatively, there is a tidy way to achieve the same by using [`count()`](https://dplyr.tidyverse.org/reference/count.html) (thanks [`dplyr`](https://dplyr.tidyverse.org/index.html)!):

```{r}
#| code-line-numbers: "|2"
#| output-location: column-fragment
review_data |>
  count(product)
```


## Counting Categorical Data

Including [`sort = TRUE`](https://dplyr.tidyverse.org/reference/count.html) arranges the results in descending order:

```{r}
#| code-line-numbers: "|2"
#| output-location: column-fragment
review_data |>
  count(product, sort = TRUE)
```


## Cleaning the Text

There are different methods you can use to condition the text data:

- An advanced option would be to convert the data frame to a Corpus and Document Term Matrix using the [`tm`](https://tm.r-forge.r-project.org/) text mining package and then use the `tm_map()` function to do the cleaning.

- But for this tutorial, we will be using the [`tidyverse`](https://www.tidyverse.org/) tidy principles, and the [`textclean`](https://github.com/trinker/textclean) package.


## Cleaning the Text

:::: {.columns}

::: {.column width="40%"}

![](img/textclean.png){width="400" height="100" fig-align="center"}

:::

::: {.column width="60%"}

[textclean](https://github.com/trinker/textclean) is a package containing several functions that automate the

- checking, 
- cleaning, and 
- normalization of text.

::: {.fragment}

Can be installed by:

````r
install.packages("textclean")
````

:::

::: {.fragment}

Once installed, it can be loaded as usual:

```{r}
library(textclean)
```

:::

:::

::::


## Cleaning the Text

The `check_text()` function performs a thorough analysis of the text, suggesting any pre-processing ought to be done (be ready for a long output!):

```{r}
#| output-location: slide
check_text(review_data$review)
```


## Cleaning the Text

We can see that [`textclean`](https://github.com/trinker/textclean) has identified several pre-processing suggestions and solutions:

- **Contractions**: `replace_contraction()` function to replace any contractions with their multi-word forms (i.e., *wasn't* to *was not*, *i'd* to *i would*, etc.)
- **Date**: `replace_date()` with `replacement = ""` to replace any date with a blank character.
- **Time**: `replace_time()` with `replacement = ""` to replace any time with a blank character.
- **Emojis**: `replace_emoji()` to replace any emoji (i.e., 👌) with word equivalents.
- **Emoticons**: `replace_emoticon()` to replace any emoticon (i.e., *;)* ) with word equivalents.
- **Hashtags**: `replace_hash()` to replace any *#hashtag* with a blank character.


## Cleaning the Text

- **Numbers**: `replace_number()` to replace any number (including coma separated numbers) with a blank character.
- **HTML**: `replace_html()` with `symbol = FALSE` to remove any HTML markup.
- **Incomplete Sentences**: `replace_incomplete()` with `replacement = ""` to replace incomplete sentence end marks (i.e. *...*).
- **URL**: `replace_url()` with `replacement = ""` to replace any URL with a blank character.
- **Kern**: `replace_kern()` to remove any added manual space (i.e., *The B O M B !* to *The BOMB!*).
- **Internet Slang**: `replace_internet_slang()` to replace the slang with longer word equivalents (i.e., *ASAP* to *as soon as possible*).


## Cleaning the Text

Traditionally, we would need to call every function one at a time:

````r
review_data$review <- replace_contraction(review_data$review)

review_data$review <- replace_date(review_data$review, replacement = "")

review_data$review <- replace_time(review_data$review, replacement = "")

...
````


## Cleaning the Text

The benefit of using the `|>` is very apparent in situations like this:

```{r}
#| code-line-numbers: "|1|2,3,4,5,6,7,8,9,10,11,12"
review_data$review <- review_data$review |>
  replace_date(replacement = "") |>
  replace_time(replacement = "") |>
  replace_email() |>
  replace_emoticon() |>
  replace_number() |>
  replace_html(symbol = FALSE) |>
  replace_incomplete(replacement = "") |>
  replace_url(replacement = "") |>
  replace_kern() |>
  replace_internet_slang() |>
  replace_contraction() 
```


## Cleaning the Text

In addition, we can use [`str_remove_all()`](https://stringr.tidyverse.org/reference/str_remove.html) from the [`stringr`](https://stringr.tidyverse.org/) package (part of the [`tidyverse`](https://www.tidyverse.org/)) to remove all matched patterns from a string:

```{r}
review_data$review <- review_data$review |>
  str_remove_all("&#34;")
```


## Using `tidytext`

Once our text has been cleaned, we will be using [`tidytext`](https://juliasilge.github.io/tidytext/) to preprocess text data.

:::: {.columns}

::: {.column width="40%"}

![](img/tidytext.png){width=400 height=400 fig-align="center"}

:::

::: {.column width="60%"}

::: {.fragment}

As before, we can install this package by:

````r
install.package("tidytext")
````

:::

::: {.fragment}

And we will load it with:

```{r}
library(tidytext)
```

:::

:::

::::


## Tokenizing Text

- Text mining or text analysis methods are based on counting:

  - words,
  - phrases,
  - sentences, or
  - any other meaningful segment.

- These segments are called **tokens**.


## Tokenizing Text

- Therefore, we need to

  - break out the reviews into individual words (or tokens) and 
  - begin mining for insights. 

- This process is called **tokenization**.


## Tokenization with [`tidytext`](https://juliasilge.github.io/tidytext/)

- From a `tidy text` framework, we need to both 

  - break the text into individual tokens (tokenization) and 
  - transform it to a `tidy` data structure.

- `tidy text` is defined as a **one-token-per-row** dataframe, where a token can be

  - a character, 
  - a word, 
  - an n-gram, 
  - a sentence, 
  - a paragraph, 
  - a tweet, 
  - etc.


## Tokenization with [`tidytext`](https://juliasilge.github.io/tidytext/)

- We can do this by using [`unnest_tokens()`](https://juliasilge.github.io/tidytext/reference/unnest_tokens.html) from [`tidytext`](https://juliasilge.github.io/tidytext/).

- [`unnest_tokens()`](https://juliasilge.github.io/tidytext/reference/unnest_tokens.html) requires at least two arguments:

  - the *output column name* that will be created as the text is unnested into it (`word` in our case, for simplicity), and
  - the *input column* that hold the current text (i.e., `review` in our case)


## Tokenization with [`tidytext`](https://juliasilge.github.io/tidytext/) {auto-animate=true}

```{r}
#| code-line-numbers: "|2"
tidy_review <- review_data |>
  unnest_tokens(word, review)
```


## Tokenization with [`tidytext`](https://juliasilge.github.io/tidytext/) {auto-animate=true}

```{r}
#| output-location: fragment
tidy_review <- review_data |>
  unnest_tokens(word, review)

tidy_review
```


## Counting words (or tokens)

```{r}
#| code-line-numbers: "|2"
#| output-location: fragment
tidy_review |>
  count(word, sort = TRUE)
```


## Removal of stop words

- Stop words are overly common words that may not add any meaning to our results (e.g., *"the"*, *"have"*, *"is"*, *"are"*). 

- We want to exclude them from our textual data and our analysis completely.

- There is no single universal list of stop words.

- Nor any agreed upon rules for identifying stop words!

- Luckily, there are several different lists to choose from...


## Removal of stop words

We can get a specific stop word lexicon via the [`stopwords()`](https://stopwords.quanteda.io/reference/stopwords.html) function from the [`stopwords`](https://stopwords.quanteda.io/) package, in a `tidy` format with one word per row.

. . .

We first need to install the package:
````r
install.package("stopwords")
````

. . .

Followed by loading it:
```{r}
library(stopwords)
```

. . . 

Then we can obtain the different sources for stop words:
```{r}
#| output-location: fragment
stopwords_getsources()
```


## Removal of stop words

Stop words lists are available in multiple languages too! 

. . .

```{r}
#| output-location: fragment
stopwords_getlanguages("snowball")
```

. . .

```{r}
#| output-location: fragment
stopwords_getlanguages("marimo")
```

. . .

```{r}
#| output-location: fragment
stopwords_getlanguages("stopwords-iso")
```


## Removal of stop words

Different word lists contain different words!

. . .

```{r}
#| output-location: fragment
get_stopwords(language = "en", source = "snowball") |>
  count()
```

. . .

```{r}
#| output-location: fragment
get_stopwords(language = "en", source = "stopwords-iso") |>
  count()
```

. . .

```{r}
#| output-location: fragment
get_stopwords(language = "en", source = "smart") |>
  count()
```


## Removal of stop words

We can sample a random list of these stop words.

By default, from `smart` in English (en).

```{r}
#| output-location: fragment
head(sample(stop_words$word, 15), 15)
```


## Removal of stop words

:::: {.columns}

::: {.column width="40%"}

![](img/dplyr-anti-join.png){width=400 height=200 fig-align="center"}

:::

::: {.column width="60%"}

::: {.fragment}

To remove stops words from our tidy tibble using [`tidytext`](https://juliasilge.github.io/tidytext/), we will use a join.

:::

::: {.fragment}

After we tokenize the reviews into words, we can use [`anti_join()`](https://dplyr.tidyverse.org/reference/filter-joins.html) to remove stop words. 

:::

:::

::::

. . .

```{r}
#| code-line-numbers: "|2|3"
tidy_review <- review_data |>
  unnest_tokens(word, review) |>
  anti_join(stop_words)
```


## Removal of stop words

If we want to select another source or another language, we can join using the [`get_stopwords()`](https://juliasilge.github.io/tidytext/reference/get_stopwords.html) function directly:

````r
tidy_review_clean <- review_data |>
  unnest_tokens(word, review) |>
  anti_join(tidytext::get_stopwords(language = 'es', source = 'stopwords-iso'))
````

## Removal of stop words

:::: {.columns}

::: {.column width="50%"}

Notice that `stop_words` already has a `word` column.

::: {.fragment}

```{r}
#| output-location: fragment
stop_words
```

:::

:::

::: {.column width="50%"}

::: {.fragment}

and a new column called `word` was created by the [`unest_tokens()`](https://juliasilge.github.io/tidytext/reference/unnest_tokens.html) function,

:::

::: {.fragment}

```{r}
#| output-location: fragment
review_data |>
  unnest_tokens(word, review)
```

:::

:::

::::


## Removal of stop words

so [`anti_join()`](https://dplyr.tidyverse.org/reference/filter-joins.html) automatically joins on the column `word`.

. . .

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: false
review_data |>
  unnest_tokens(word, review)
```

:::

::: {.column width="50%"}

```{r}
#| echo: false
review_data |>
  unnest_tokens(word, review) |>
  anti_join(stop_words)
```

:::

::::


## Removal of stop words

Let's check the result.

```{r}
#| code-line-numbers: "|2"
#| output-location: fragment
tidy_review |>
  count(word, sort = TRUE)
```


## Plotting Word Counts

:::: {.columns}

::: {.column width="40%"}

![](img/ggplot2.png){width=300 height=300 fig-align="center"}

:::

::: {.column width="60%"}

::: {.fragment}

We will use [`ggplot2`](https://ggplot2.tidyverse.org/) for the data visualization.

:::

::: {.fragment}

The library is automatically loaded as part of [`tidyverse`](https://www.tidyverse.org/), but can be called separately:

```{r}
library(ggplot2)
```

:::

:::

::::


## Plotting Word Counts {auto-animate=true}

Starting with our `tidy text`, we want to create an extra column called `id` to be able to identify the review.

```{r}
#| code-line-numbers: "|2|3|4"
tidy_review <- review_data |>
  mutate(id = row_number()) |>
  unnest_tokens(word, review) |>
  anti_join(stop_words)
```


## Plotting Word Counts {auto-animate=true}

Starting with our `tidy text`, we want to create an extra column called `id` to be able to identify the review.

```{r}
#| output-location: fragment
tidy_review <- review_data |>
  mutate(id = row_number()) |>
  unnest_tokens(word, review) |>
  anti_join(stop_words)

tidy_review
```


## Plotting Word Counts {auto-animate=true}

Visualizing counts with [`geom_col()`](https://ggplot2.tidyverse.org/reference/geom_bar.html):

```{r}
word_counts <- tidy_review |>
  count(word, sort = TRUE)
```


## Plotting Word Counts {auto-animate=true}

Visualizing counts with [`geom_col()`](https://ggplot2.tidyverse.org/reference/geom_bar.html):

```{r}
#| code-line-numbers: "|4,5,6,7,8,9,10,11"
#| output-location: fragment
#| fig-width: 16
word_counts <- tidy_review |>
  count(word, sort = TRUE)

word_counts |>
  ggplot(
    aes(
      x = word,
      y = n
    )
  ) +
  geom_col()
```


## Plotting Word Counts

We can combine using the pipe `|>` to make it easier to read and more concise!

```{r}
#| code-line-numbers: "|2|3,4,5,6,7,8"
#| output-location: fragment
#| fig-width: 16
tidy_review |>
  count(word, sort = TRUE) |>
  ggplot(
    aes(
      x = word, 
      y = n)
    ) +
  geom_col()
```


## Plotting Word Counts {auto-animate=true}

Too many words? We can [`filter()`](https://dplyr.tidyverse.org/reference/filter.html) before visualizing:

```{r}
#| code-line-numbers: "|2|3|4"
word_counts_filter <- tidy_review |>
  count(word) |>
  filter(n > 100) |>
  arrange(desc(n))
```


## Plotting Word Counts {auto-animate=true}

Too many words? We can [`filter()`](https://dplyr.tidyverse.org/reference/filter.html) before visualizing:

```{r}
#| output-location: fragment
word_counts_filter <- tidy_review |>
  count(word) |>
  filter(n > 100) |>
  arrange(desc(n))

word_counts_filter
```


## Plotting Word Counts

We can do a few tweaks to improve the count visualization.

```{r}
#| code-line-numbers: "|1|2,3,4,5,6,7|8|9|10,11,12"
#| output-location: column-fragment
#| fig-height: 6
word_counts_filter |>
  ggplot(
    aes(
      x = word,
      y = n
    )
  ) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Review Word Counts"
  )
```


## Plotting Word Counts

Again, we can pipe everything together using `|>` to make it more concise:

```{r}
#| code-line-number: "|1,2,3,4|5,6,7,8,9,10,11,12,13"
#| output-location: column-fragment
#| fig-height: 6
tidy_review |>
  count(word) |>
  filter(n > 100) |>
  arrange(desc(n)) |>
  ggplot(
    aes(
      x = word, 
      y = n)
    ) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Review Word Counts"
  )
```


## Adding Custom Stop Words

Sometimes, we discover a number of words in the data that aren't informative and should be removed from our final list of words:

. . .

```{r}
#| code-line-numbers: "|2"
#| output-location: fragment
tidy_review |>
  filter(word == "yr")
```

. . .

We will add a few words to our `custom_stop_words` data frame.


## Adding Custom Stop Words

Firstly, let's look at the structure of the `stop_words` data frame:

```{r}
#| output-location: fragment
stop_words
```


## Adding Custom Stop Words {auto-animate=true}

For that, we can create a custom tibble/data frame called `custom_stop_words`.

. . . 

The column names of the new data frame of custom stop words should match `stop_words` (i.e., `~word` and `~lexicon`).

. . . 

```{r}
#| code-line-numbers: "|1|2|3,4,5,6,7"
#| output-location: column-fragment
custom_stop_words <- tribble(
  ~word, ~lexicon,
  "madlibs", "CUSTOM",
  "cd's", "CUSTOM",
  "yr", "CUSTOM"
)
```


## Adding Custom Stop Words {auto-animate=true}

For that, we can create a custom tibble/data frame called `custom_stop_words`.

. . . 

The column names of the new data frame of custom stop words should match `stop_words` (i.e., `~word` and `~lexicon`).

. . . 

```{r}
#| output-location: column-fragment
custom_stop_words <- tribble(
  ~word, ~lexicon,
  "madlibs", "CUSTOM",
  "cd's", "CUSTOM",
  "yr", "CUSTOM"
)

custom_stop_words
```


## Adding Custom Stop Words

We can now merge both lists into one that we can use for the analysis by using [`bind_rows()`](https://dplyr.tidyverse.org/reference/bind_rows.html):

```{r}
#| code-line-numbers: "|2"
stop_words_new <- stop_words |>
  bind_rows(custom_stop_words)
```


## Adding Custom Stop Words

After that, we can use it with [`anti_join()`](https://dplyr.tidyverse.org/reference/filter-joins.html) to remove all the stop words at once!

```{r}
#| code-line-numbers: "|3"
tidy_review <- review_data |>
  unnest_tokens(word, review) |>
  anti_join(stop_words_new)
```


## Adding Custom Stop Words

We can combine all the steps together now:

```{r}
#| code-line-numbers: "|1|2|3|4|5"
tidy_review <- review_data |>
  mutate(id = row_number()) |>
  select(id, date, product, stars, review) |>
  unnest_tokens(word, review) |>
  anti_join(stop_words_new)
```


## Adding Custom Stop Words

Let's check if that word is still there...

```{r}
#| code-line-numbers: "|2"
#| output-location: fragment
tidy_review |>
  filter(word == "yr")
```


## Back to Plotting Word Counts

We are still able to pipe it all together with `|>` :scream_cat:

```{r}
#| code-line-numbers: "|2|3|4,5,6,7|8,9,10,11,12|13|14|15,16,17"
#| output-location: column-fragment
#| fig-height: 6
review_data |>
  mutate(id = row_number()) |>
  select(id, date, product, stars, review) |>
  unnest_tokens(word, review) |>
  anti_join(stop_words_new) |>
  count(word) |>
  filter(n > 100) |>
  ggplot(
    aes(
      x = word, 
      y = n)
    ) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Review Word Counts"
  )
```


## (Still) Improving the Count Visualization {auto-animate=true}

To order the different words (i.e., tokens), we can use the [`fct_reorder()`](https://forcats.tidyverse.org/reference/fct_reorder.html) function from [`forcats`](https://forcats.tidyverse.org/index.html), also part of [`tidyverse`](https://www.tidyverse.org/):

```{r}
#| code-line-numbers: "|2|3|4"
word_counts <- tidy_review |>
  count(word) |>
  filter(n > 100) |>
  mutate(word2 = fct_reorder(word, n))
```


## (Still) Improving the Count Visualization {auto-animate=true}

To order the different words (i.e., tokens), we can use the [`fct_reorder()`](https://forcats.tidyverse.org/reference/fct_reorder.html) function (or `reorder()`) from [`forcats`](https://forcats.tidyverse.org/index.html), also part of [`tidyverse`](https://www.tidyverse.org/):

```{r}
#| output-location: fragment
word_counts <- tidy_review |>
  count(word) |>
  filter(n > 100) |>
  mutate(word2 = fct_reorder(word, n))

word_counts
```


## (Still) Improving the Count Visualization

That way, we have a better looking bar plot.

```{r}
#| code-line-numbers: "|1|2,3,4,5,6,7|8|9|10,11,12"
#| output-location: column-fragment
#| fig-height: 6
word_counts |>
  ggplot(
    aes(
      x = word2,
      y = n
    )
  ) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Review Word Counts"
  )
```


## (Still) Improving the Count Visualization

Now, by product!

```{r}
#| output-location: fragment
tidy_review |>
  count(word, product, sort = TRUE)
```


## (Still) Improving the Count Visualization

Better to [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html):

```{r}
#| code-line-numbers: "|3"
#| output-location: fragment
tidy_review |>
  count(word, product) |>
  group_by(product)
```


## (Still) Improving the Count Visualization

Using [`slice_max()`](https://dplyr.tidyverse.org/reference/slice.html) allows us to select the largest values of a variable:

```{r}
#| code-line-numbers: "|4"
#| output-location: fragment
tidy_review |>
  count(word, product) |>
  group_by(product) |>
  slice_max(n, n = 10)
```


## (Still) Improving the Count Visualization {auto-animate=true}

We will use [`ungroup()`](https://dplyr.tidyverse.org/reference/group_by.html) to remove the groups.

Followed by [`fct_reorder()`](https://forcats.tidyverse.org/reference/fct_reorder.html).

```{r}
#| code-line-numbers: "|5|6"
word_counts <- tidy_review |>
  count(word, product) |>
  group_by(product) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  mutate(word2 = fct_reorder(word, n))
```


## (Still) Improving the Count Visualization {auto-animate=true}

We will use [`ungroup()`](https://dplyr.tidyverse.org/reference/group_by.html) to remove the groups.

Followed by [`fct_reorder()`](https://forcats.tidyverse.org/reference/fct_reorder.html).

```{r}
#| output-location: fragment
word_counts <- tidy_review |>
  count(word, product) |>
  group_by(product) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  mutate(word2 = fct_reorder(word, n))

word_counts
```


## (Still) Improving the Count Visualization

To visualize, we need to use [`facet_wrap()`](https://ggplot2.tidyverse.org/reference/facet_wrap.html), which allows us to "split" the graph by a determined `facet`:


```{r}
#| code-line-numbers: "|2|3,4,5,6,7,8|9|10|11|12,13,14|"
#| output-location: slide
#| fig-width: 15
#| fig-height: 7
word_counts |>
  ggplot(
    aes(
      x = word2,
      y = n,
      fill = product
    )
  ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~product, scales = "free_y") +
  coord_flip() +
  labs(
    title = "Review Word Counts"
  )
```


## (Still) Improving the Count Visualization

As explained before, we can `|>` our way across the code:

````r
tidy_review |>
  count(word, product) |>
  group_by(product) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  mutate(word2 = fct_reorder(word, n)) |>
  ggplot(
    aes(
      x = word2,
      y = n,
      fill = product
    )
  ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~product, scales = "free_y") +
  coord_flip() +
  labs(
    title = "Review Word Counts"
  )
````


## Creating Word Clouds

- A `word cloud` is a visual representation of text data.

- It is often used to visualize free form text. 

- They are usually composed of single words.

- The importance of each tag is shown with font size or color.


## Creating Word Clouds

There are several alternative packages to generate word clouds in R.

. . .

For this workshop, we will use the [`ggwordcloud`](https://lepennec.github.io/ggwordcloud/index.html) package, as it follows `ggplot2` syntax.

. . .

It needs to be installed, following the normal procedure:

````r
install.packages("ggwordcloud")
````
. . .

After that, we need to load it before usage:

```{r}
library(ggwordcloud)
```


## Creating Word Clouds

We will use the previously created `word_count_filter` containing the words with more than 100 mentions.

. . . 

On the most basic level, we only need to use the `geom_text_wordcloud()` function for our `ggplot` plot:

```{r}
#| code-line-numbers: "|8"
#| output-location: column-fragment
#| fig-width: 5
#| fig-height: 5
set.seed(123)
word_counts_filter |>
  ggplot(
    aes(
      label = word
      )
     ) +
  geom_text_wordcloud()
```


## Creating Word Clouds

That seems to create a rather ugly word cloud. 

. . .

We can improve it by piping a theme (i.e.,  `theme_minimal()`). 

. . .

This theme displays the words and nothing else. 

```{r}
#| code-line-numbers: "|9"
#| output-location: column-fragment
#| fig-width: 6
#| fig-height: 6
set.seed(123)
word_counts_filter |>
  ggplot(
    aes(
      label = word
      )
     ) +
  geom_text_wordcloud() +
  theme_minimal()
```


## Creating Word Clouds

So far, all the words are the same size.

. . .

We can introduce the total count of words that we have already calculated as the size.

```{r}
#| code-line-numbers: "|6"
#| output-location: column-fragment
#| fig-width: 6
#| fig-height: 6
set.seed(123)
word_counts_filter |>
  ggplot(
    aes(
      label = word, 
      size = n
      )
     ) +
  geom_text_wordcloud() +
  theme_minimal()
```


## Creating Word Clouds

To obtain better proportionality, we need to use [`scale_size_area()`](https://ggplot2.tidyverse.org/reference/scale_size.html):

```{r}
#| code-line-numbers: "|11"
#| output-location: column-fragment
#| fig-width: 6
#| fig-height: 6
set.seed(123)
word_counts_filter |>
  ggplot(
    aes(
      label = word, 
      size = n
      )
     ) +
  geom_text_wordcloud() +
  theme_minimal() +
  scale_size_area(max_size = 20)
```


## Creating Word Clouds

If we want a tighter knitted word cloud with more exagerated sizes, we can use `scale_radius()` instead:

```{r}
#| code-line-numbers: "|11,12"
#| output-location: column-fragment
#| fig-width: 6
#| fig-height: 6
set.seed(123)
word_counts_filter |>
  ggplot(
    aes(
      label = word,
      size = n
      )
     ) +
  geom_text_wordcloud() +
  theme_minimal() +
  scale_radius(range = c(0, 30), 
               limits = c(0, NA))
```


## Creating Word Clouds

`ggwordcloud` also allows us to change the shape of our word cloud, by using `geom_text_wordcloud_area(shape = shape)`:

```{r}
#| code-line-numbers: "|9"
#| output-location: column-fragment
#| fig-width: 6
#| fig-height: 6
set.seed(123)
word_counts_filter |>
  ggplot(
    aes(
      label = word,
      size = n
      )
     ) +
  geom_text_wordcloud_area(shape = "pentagon") +
  theme_minimal() +
  scale_radius(range = c(0, 30),
               limits = c(0, NA))
```


## Creating Word Clouds

Finally, we can apply some colour to our word cloud:

```{r}
#| code-line-numbers: "|7|13"
#| output-location: column-fragment
#| fig-width: 6
#| fig-height: 6
set.seed(123)
word_counts_filter |>
  ggplot(
    aes(
      label = word,
      size = n,
      color = n
      )
     ) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 20) +
  theme_minimal() +
  scale_color_gradient2()
```


## Creating Word Clouds by Star Review {auto-animate=true}

We can first group by stars using [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html) and then [`filter()`](https://dplyr.tidyverse.org/reference/filter.html) by the desired rating:

```{r}
#| code-line-numbers: "|4|5"
set.seed(13)

word_counts_stars <- tidy_review |>
  group_by(stars) |>
  filter(stars == 1) |>
  count(word) |>
  filter(n > 5) |>
  arrange(desc(n))
```


## Creating Word Clouds by Star Review {auto-animate=true}

We can first group by stars using [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html) and then [`filter()`](https://dplyr.tidyverse.org/reference/filter.html) by the desired rating:

```{r}
#| output-location: column-fragment
#| fig-width: 6
#| fig-height: 6
set.seed(13)

word_counts_stars <- tidy_review |>
  group_by(stars) |>
  filter(stars == 1) |>
  count(word) |>
  filter(n > 5) |>
  arrange(desc(n))

word_counts_stars |>
  ggplot(
    aes(
      label = word,
      size = n,
      color = n
      )
     ) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 20) +
  theme_minimal() +
  scale_color_gradient2()
```


## Creating Word Clouds by Product Review {auto-animate=true}

We can first filter by the desired product using [`filter()`](https://dplyr.tidyverse.org/reference/filter.html):

```{r}
#| code-line-numbers: "|4"
set.seed(13)

word_counts_product <- tidy_review |>
  filter(product == "Charcoal Fabric") |>
  count(word) |>
  filter(n > 5) |>
  arrange(desc(n))
```


## Creating Word Clouds by Product Review {auto-animate=true}

We can first filter by the desired product using [`filter()`](https://dplyr.tidyverse.org/reference/filter.html):

```{r}
#| output-location: column-fragment
#| fig-width: 6
#| fig-height: 6
set.seed(13)

word_counts_product <- tidy_review |>
  filter(product == "Charcoal Fabric") |>
  count(word) |>
  filter(n > 5) |>
  arrange(desc(n))

word_counts_product |>
  ggplot(
    aes(
      label = word,
      size = n,
      color = n
      )
     ) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 20) +
  theme_minimal() +
  scale_color_gradient2()
```


# Part Two: Tidy Sentiment Analysis in R

Through the looking-glass


## Sentiment Analysis Overview

- In the previous chapter, we explored in depth what we mean by the `tidy text` format and showed how this format can be used to approach questions about word frequency. 

- This allowed us to analyze which words are used most frequently in documents and to compare documents. 

- Let’s now address the topic of opinion mining or sentiment analysis. 

- When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive :+1: or negative :-1: , or perhaps characterized by some other more nuanced emotion like surprise :astonished: or confusion :confused:. 


## Sentiment Analysis - Levels

- As we have previously explored, different levels of analysis based on the text are possible:

  - document, 
  - sentence, and 
  - word. 

- In addition, more complex documents can also have dates, volumes, chapters, etc. 


## Sentiment Analysis - Levels

- Word level analysis exposes detailed information and can be used as foundational knowledge for more advanced practices in topic modeling.

- Therefore, a way to analyze the sentiment of a text is

  - to consider the text as a combination of its individual words and 
  - the sentiment content of the whole text as the sum of the sentiment content of the individual words. 

- *This* is an often-used approach, *and* an approach that naturally takes advantage of the `tidy` tool ecosystem.


## Sentiment Analysis - Methods

- There are different methods used for sentiment analysis, including:

  - training a known dataset, 
  - creating your own classifiers with rules, and 
  - using predefined lexical dictionaries (lexicons). 

- In this tutorial, you will use the *lexicon-based approach*, but I would encourage you to investigate the other methods as well as their associated trade-offs.


## Sentiment Analysis - Dictionaries

- Several distinct dictionaries exist to evaluate the opinion or emotion in text. 

- The [`tidytext`](https://juliasilge.github.io/tidytext/index.html) package provides access to several sentiment lexicons, using the [`get_sentiments()`](https://juliasilge.github.io/tidytext/reference/get_sentiments.html) function:

  -   **AFINN** from Finn Årup Nielsen,
  -   **bing** from Bing Liu and collaborators,
  -   **nrc** from Saif Mohammad and Peter Turney, and
  -   **loughran** from Loughran-McDonald.


## Sentiment Analysis - Dictionaries

- All four of these lexicons are based on *unigrams*, i.e., single words. 

- These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth.

- Dictionary-based methods find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text.


## Sentiment Analysis - Dictionaries

- Not every English word is present in the lexicons because many English words are pretty neutral. 

- These methods do not take into account qualifiers before a word, such as in “no good” or “not true”; a lexicon-based method like this is based on unigrams only.

- For many kinds of text (like the example in this workshop), there are not sustained sections of sarcasm or negated text, so this is not an important effect. 

- Also, we can use a `tidy text` approach to begin to understand what kinds of negation words are important in a given text.


## Sentiment Analysis  - Dictionaries

- The size of the chunk of text that we use to add up unigram sentiment scores can have an effect on an analysis. 

- A text the size of many paragraphs can often have positive and negative sentiment averaged out to about zero, while sentence-sized or paragraph-sized text often works better.

- An example of sentence-based analysis using the [`sentimentr`](https://github.com/trinker/sentimentr) package is included in the Appendix (for those who are impatient!).


## AFINN Dictionary

The [AFINN lexicon](https://www2.imm.dtu.dk/pubdb/pubs/6010-full.html) [@afinn_lexicon] can be loaded by using the [`get_sentiments()`](https://juliasilge.github.io/tidytext/reference/get_sentiments.html) function:

```{r}
#| output-location: fragment
get_sentiments("afinn")
```


## AFINN Dictionary

The [AFINN lexicon](https://www2.imm.dtu.dk/pubdb/pubs/6010-full.html) [@afinn_lexicon] assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

```{r}
#| code-line-numbers: "|2,3,4,5"
#| output-location: fragment
get_sentiments("afinn") |>
  summarize(
    min = min(value),
    max = max(value)
  )
```


## Bing Dictionary

The [bing lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) [@bing_lexicon] categorizes words in a binary fashion into "positive" and "negative" categories.

```{r}
#| code-line-numbers: "|1|2"
#| output-location: fragment
get_sentiments("bing") |>
  count(sentiment)
```


## nrc Dictionary

The [nrc lexicon](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) [@nrc_lexicon] categorizes words in a binary fashion (“yes”/“no”) into categories of "positive", "negative", "anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", and "trust".

```{r}
#| code-line-numbers: "|1|2"
#| output-location: fragment
get_sentiments("nrc") |>
  count(sentiment, sort = TRUE)
```


## Loughran dictionary

The [Loughran lexicon](https://sraf.nd.edu/loughranmcdonald-master-dictionary/) [@loughran_lexicon] was created for use with financial documents, and labels words with six possible sentiments important in financial contexts: "negative", "positive", "litigious", "uncertainty", "constraining", or "superfluous".

````r
sentiment_loughran <- get_sentiments("loughran") |>
  count(sentiment) |>
  mutate(sentiment2 = fct_reorder(sentiment, n))

sentiment_loughran |>
  ggplot(
    aes(
      x = sentiment2,
      y = n
    )
  ) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Sentiment Counts in Loughran",
    x = "Counts",
    y = "Sentiment"
  )
````


## Using Dictionaries

:::: {.columns}

::: {.column width="40%"}

::: {.fragment}

![](img/dplyr-inner-join.png){width=500 length=300 fig-align="center"}

:::

:::

::: {.column width="60%"}

::: {.fragment}

Dictionaries need to be appended by using [`inner_join()`](https://dplyr.tidyverse.org/reference/mutate-joins.html).

:::

::: {.fragment}

The function drops any row in *either* data set that does not have a match in both data sets.

:::

:::

::::

. . .

```{r}
#| code-line-numbers: "|2"
#| output-location: fragment
tidy_review |>
  inner_join(get_sentiments("nrc"))
```


## Counting Sentiments

After that, we can count the sentiments.

```{r}
#| code-line-numbers: "|3"
#| output-location: fragment
tidy_review |>
  inner_join(get_sentiments("nrc")) |>
  count(sentiment)
```


## Counting Sentiments

We can also count how many words are linked to which sentiment.

```{r}
#| code-line-numbers: "|3,4"
#| output-location: fragment
tidy_review |>
  inner_join(get_sentiments("nrc")) |>
  count(word, sentiment) |>
  arrange(desc(n))
```


## Visualizing Sentiments

We will focus only in positive and negative sentiments.

```{r}
#| code-line-numbers: "|3|5-10"
sentiment_review_viz <- tidy_review |>
  inner_join(get_sentiments("nrc")) |>
  filter(sentiment %in% c("positive", "negative"))

word_counts <- sentiment_review_viz |>
  count(word, sentiment) |>
  group_by(sentiment) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  mutate(word2 = fct_reorder(word, n))
```


## Visualizing Sentiments

We will focus only in positive and negative sentiments.

```{r}
#| output-location: slide
#| fig-height: 7
#| fig-width: 16
word_counts |>
  ggplot(
    aes(
      x = word2,
      y = n,
      fill = sentiment
      )
    ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free") +
  coord_flip() +
  labs(
    title = "Sentiment Word Counts (nrc lexicon)",
    x = "Words"
  )
```


## Visualizing Sentiments

Of course, we can `tidy` and `|>` all that code :smirk:

````r
tidy_review |>
  inner_join(get_sentiments("nrc")) |>
  filter(sentiment %in% c("positive", "negative")) |>
  count(word, sentiment) |>
  group_by(sentiment) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  mutate(word2 = fct_reorder(word, n)) |>
  ggplot(
    aes(
    x = word2,
    y = n,
    fill = sentiment
    )
   ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free") +
  coord_flip() +
  labs(
    title = "Sentiment Word Counts (nrc lexicon)",
    x = "Words"
  )
````


## Counting Sentiment by Star Rating

Let's use the bing lexicon for this experiment.

```{r}
#| code-line-numbers: "|2|3"
#| output-location: fragment
tidy_review |>
  inner_join(get_sentiments("bing")) |>
  count(stars, sentiment)
```


## Counting Sentiment by Star Rating

For a more comfortable exploration, we may want to transpose the results. 

That can be achieved with the [`pivot_wider()`](https://tidyr.tidyverse.org/reference/pivot_wider.html) function (from [`tidyr`](https://tidyr.tidyverse.org/index.html)), which will transform data from long to wide format.

```{r}
#| code-line-numbers: "|4"
#| output-location: fragment
tidy_review |>
  inner_join(get_sentiments("bing")) |>
  count(stars, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n)
```


## Computing Overall Sentiment by Star Rating

After that, we can use [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html) to create a new column with the overall sentiment rating:

```{r}
#| code-line-numbers: "|5"
#| output-location: fragment
tidy_review |>
  inner_join(get_sentiments("bing")) |>
  count(stars, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n) |>
  mutate(overall_sentiment = positive - negative)
```


## Visualizing Sentiment by Star Rating

We can put it all together to obtain a visualization :tada:

```{r}
#| code-line-numbers: "|5,6,7,8|9,10,11,12,13,14,15|16|17|18-23"
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
tidy_review |>
  inner_join(get_sentiments("bing")) |>
  count(stars, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n) |>
  mutate(
    overall_sentiment = positive - negative,
    stars2 = reorder(stars, overall_sentiment)
         ) |>
  ggplot(
       aes(
         x = stars2,
         y = overall_sentiment,
         fill = as.factor(stars)
         )
       ) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Overall Sentiment by Star rating (bing lexicon)",
    subtitle = "Reviews for Alexa",
    x = "Stars",
    y = "Overall Sentiment"
  )
```


## Most Common Positive and Negative Words 

- One advantage of having the data frame with both sentiment and word is that we can analyze word counts that contribute to each sentiment. 

- By implementing [`count()`](https://dplyr.tidyverse.org/reference/count.html) here with arguments of both `word` and `sentiment`, we find out how much each word contributed to each sentiment.


## Most Common Positive and Negative Words {auto-animate=true}

```{r}
#| code-line-numbers: "|2|3|4"
bing_word_counts <- tidy_review |>
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE)
```


## Most Common Positive and Negative Words {auto-animate=true}

```{r}
#| output-location: fragment
bing_word_counts <- tidy_review |>
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE)

bing_word_counts
```


## Most Common Positive and Negative Words

This can be shown visually, and we can pipe straight into [`ggplot2`](https://ggplot2.tidyverse.org/index.html), if we like, because of the way we are consistently using tools built for handling `tidy` data frames:

```{r}
#| code-line-numbers: "|2|3|4|5|6,7,8,9,10,11,12|13|14|15,16,17,18"
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
bing_word_counts |>
  group_by(sentiment) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  mutate(word = reorder(word, n)) |>
  ggplot(
    aes(
      x = n,
      y = word,
      fill = sentiment
      )
    ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(
    x = "Contribution to sentiment",
    y = NULL
    )
```


## Most Common Positive and Negative Words (by Star Rating) {auto-animate=true}

We can do the same, but slicing the data by the star rating gave by the consumers using [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html):

```{r}
#| code-line-numbers: "|2"
bing_word_counts_by_stars <- tidy_review |>
  group_by(stars) |>
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE) |>
  ungroup()
```


## Most Common Positive and Negative Words (by Star Rating) {auto-animate=true}

We can do the same, but slicing the data by the star rating gave by the consumers using [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html):

```{r}
#| output-location: fragment
bing_word_counts_by_stars <- tidy_review |>
  group_by(stars) |>
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE) |>
  ungroup()

bing_word_counts_by_stars
```


## Most Common Positive and Negative Words (by Star Rating)

We can focus on 1 star rating using [`filter()`](https://dplyr.tidyverse.org/reference/filter.html):

```{r}
#| code-line-numbers: "|2"
#| output-location: slide
#| fig-width: 16
#| fig-height: 7
bing_word_counts_by_stars |>
  filter(stars == 1) |>
  group_by(sentiment) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  mutate(word = reorder(word, n)) |>
  ggplot(
    aes(
      x = n,
      y = word,
      fill = sentiment
      )
    ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(
    x = "Contribution to sentiment for 1 star reviews",
    y = NULL
    )
```


## Most Common Positive and Negative Words (by Star Rating)

Let's see now the 5 star reviews:

```{r}
#| code-line-numbers: "|2"
#| output-location: slide
#| fig-width: 16
#| fig-height: 7
bing_word_counts_by_stars |>
  filter(stars == 5) |>
  group_by(sentiment) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  mutate(word = reorder(word, n)) |>
  ggplot(
    aes(
      x = n, 
      y = word,
      fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(
    x = "Contribution to sentiment for 5 star reviews",
    y = NULL
    )
```


## Most Common Positive and Negative Words (by Star Rating)

Let's compare side by side

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 8
tidy_review |>
  group_by(stars) |>
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE) |>
  ungroup() |>
  filter(stars == 1) |>
  group_by(sentiment) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  mutate(word = reorder(word, n)) |>
  ggplot(
    aes(
      x = n,
      y =  word,
      fill = sentiment
      )
    ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(
    x = "Contribution to sentiment for 1 star reviews",
    y = NULL
    )
```
:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 8
tidy_review |>
  group_by(stars) |>
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE) |>
  ungroup() |>
  filter(stars == 5) |>
  group_by(sentiment) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  mutate(word = reorder(word, n)) |>
  ggplot(
    aes(
      x = n, 
      y = word,
      fill = sentiment
      )
    ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(
    x = "Contribution to sentiment for 5 star reviews",
    y = NULL
    )
```

:::

::::


## More Word Clouds!

Sometimes we want to visually present positive and negative words for the same text.

. . .

We can use the [`comparison.cloud()`](https://www.rdocumentation.org/packages/wordcloud/versions/2.6/topics/comparison.cloud) function from the [`wordcloud`](https://rdocumentation.org/packages/wordcloud/versions/2.6) package.

. . . 

First we need to install the package:

````r
install.package(wordcloud)
````

. . .

After that, we load it as usual:

```{r}
library(wordcloud)
```


## More Word Clouds!

For [`comparison.cloud()`](https://www.rdocumentation.org/packages/wordcloud/versions/2.6/topics/comparison.cloud), we may need to turn the data frame into a matrix with the `acast()` function from the [`reshape2`](https://cran.r-project.org/web/packages/reshape2/index.html) package. 

. . .

The size of a word’s text is in proportion to its frequency within its sentiment.

. . .

We can see the most important positive and negative words, but the sizes of the words are not comparable across sentiments.

. . .

```{r}
#| code-line-numbers: "|1|3|4|5|6|7,8"
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
library(reshape2)

tidy_review |>
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE) |>
  acast(word ~ sentiment, value.var = "n", fill = 0) |>
  comparison.cloud(colors = c("red", "green"),
                   max.words = 100)
```


## More Word Clouds! (by Star Rating)

We can do the same as before, focusing on the different star ratings:

```{r}
#| code-line-numbers: "|5"
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
library(reshape2)

tidy_review |>
  inner_join(get_sentiments("bing")) |>
  filter(stars == 1) |>
  count(word, sentiment, sort = TRUE) |>
  acast(word ~ sentiment, value.var = "n", fill = 0) |>
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```


## More Word Clouds! (by Star Rating)

:::: {.columns}

::: {.column width="50%"}

::: {style="font-size: 0.5em; text-align: center"}

1 star review

:::

```{r}
#| echo: false
#| fig-height: 8
library(reshape2)

tidy_review |>
  inner_join(get_sentiments("bing")) |>
  filter(stars == 1) |>
  count(word, sentiment, sort = TRUE) |>
  acast(word ~ sentiment, value.var = "n", fill = 0) |>
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

:::

::: {.column width="50%"}

::: {style="font-size: 0.5em; text-align: center"}

5 star review

:::

```{r}
#| echo: false
#| fig-height: 8
library(reshape2)

tidy_review |>
  inner_join(get_sentiments("bing")) |>
  filter(stars == 5) |>
  count(word, sentiment, sort = TRUE) |>
  acast(word ~ sentiment, value.var = "n", fill = 0) |>
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

:::

::::


# Part Three: Topic Modelling

Into the woods!


## Topic Modelling

In this last part, we will build a model using Latent Dirichlet Allocation (LDA), to give a simple example of using LDA to *generate* collections of words that together suggest themes.


## Clustering vs. Topic Modelling

- Clustering:

  -   Clusters are uncovered based on distance, which is continuous.
  -   Every object is assigned to a single cluster.

- Topic Modelling:

  -   Topics are uncovered based on word frequency, which is discrete.
  -   Every document is a mixture (i.e., partial member) of every topic.


## Topic Modelling

- Topic modelling is an unsupervised machine learning approach that can:

  - scan a collection of documents, 
  - find word and phrase patterns within them, and 
  - automatically *group* word groupings and related expressions into topics.


## Topic Modelling with LDA

- Latent Dirichlet Allocation (LDA) is a machine learning algorithm which discovers different topics underlying a collection of documents, where each document is a collection of words. 

- LDA makes the following two assumptions:

  1. Every document is a combination of one or more topic(s)
  2. Every topic is a mixture of words


## Building Models with LDA

- LDA seeks to find groups of related words. 

- It is an iterative, generative algorithm, with two main steps:

  1.  During initialization, each word is assigned to a random topic.
  2.  The algorithm goes through each word iteratively and reassigns the word to a topic with the following considerations:

    - the probability the word belongs to a topic and,
    - the probability the document can be *generated* by a topic


## Creating the Document-Term Matrix

- The LDA algorithm requires the data to be presented as a *document-term matrix (DTM)*.

- Each document is a row, and each column is a term.

. . . 

![](img/dtm.png){fig-align="center"}^[https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/ch04.html]


## Creating the Document-Term Matrix

We can achieve that by piping (`|>`) our `tidy` data to the [`cast_dtm()`](https://juliasilge.github.io/tidytext/reference/document_term_casters.html) function from `tidytext`, where:

  -   `id` is the name of the field with the document name, and
  -   `word` is the name of field with the term.

. . .

```{r}
#| code-line-numbers: "|2|3"
#| output-location: fragment
tidy_review |>
  count(word, id) |>
  cast_dtm(id, word, n)
```

. . .

This tells us how many documents and terms we have, and that this is a very sparse matrix.

. . .

The word sparse implies that the DTM contains mostly empty fields.


## Exploring the Document-Term Matrix {auto-animate=true}

We can look into the contents of a few rows and columns of the DTM by piping it into the [`as.matrix()`](https://www.rdocumentation.org/packages/data.table/versions/1.16.0/topics/as.matrix) function. 

. . .

You will see that each row is a review, and each column is a term.

. . .

```{r}
#| code-line-numbers: "|3|4"
dtm_review <- tidy_review |>
  count(word, id) |>
  cast_dtm(id, word, n) |>
  as.matrix()
```


## Exploring the Document-Term Matrix {auto-animate=true}

We can look into the contents of a few rows and columns of the DTM by piping it into the [`as.matrix()`](https://www.rdocumentation.org/packages/data.table/versions/1.16.0/topics/as.matrix) function.

. . .

You will see that each row is a review, and each column is a term.

. . .

```{r}
#| output-location: fragment
dtm_review <- tidy_review |>
  count(word, id) |>
  cast_dtm(id, word, n) |>
  as.matrix()

dtm_review[1:4, 2000:2004]
```


## Fitting the Model

We will use the [`LDA()`](https://www.rdocumentation.org/packages/topicmodels/versions/0.2-17/topics/LDA) function from the [`topicmodels`](https://rdocumentation.org/packages/topicmodels/versions/0.2-17) package.

. . .

First we need to install the package:

````r
install.packages("topicmodels")
````
. . .

And load the package as usual:

```{r}
library(topicmodels)
```


## Fitting the Model

- For our purposes, we will just need to know three parameters for the [`LDA()`](https://www.rdocumentation.org/packages/topicmodels/versions/0.2-17/topics/LDA) function:

  -   the number of topics `k` (let's start with two, so `k = 2`),
  -   the sampling method (`method =`), and
  -   the seed (for repeatable results, so `seed = 123`).


## Fitting the Model

- The `method` parameter defines the sampling algorithm to use.

- The default is `method = "VEM"`.

- We will use the `method = "Gibbs"` sampling method (it does perform better in my experience).

- An explanation of `VEM` or `Gibbs` methods is beyond this workshop, but I encourage everyone to read a bit more about these two methods.


## Fitting the Model {auto-animate=true}

Let's fit the LDA model and explore the output:

```{r}
lda_tidy <- dtm_review |> 
  LDA(
    k = 2,
    method = "Gibbs",
    control = list(seed = 123)
    )
```


## Fitting the Model {auto-animate=true}

Let's fit the LDA model and explore the output:

```{r}
#| output-location: fragment
#| code-line-numbers: "|1|2|3|4|5"
lda_tidy <- dtm_review |> 
  LDA(
    k = 2,
    method = "Gibbs",
    control = list(seed = 123)
    )

lda_tidy
```


## Exploring the `LDA()` output

If you REALLY want more details, we can use the [`glimpse()`](https://dplyr.tidyverse.org/reference/glimpse.html) function:

```{r}
#| output-location: fragment
glimpse(lda_tidy)
```


## Exploring the `LDA()` output

Most easily, we can use the [`tidy()`](https://juliasilge.github.io/tidytext/reference/lda_tidiers.html) function with the `matrix = "beta"` argument to put it into a format that is easy to understand.

Passing `beta` provides us with the *per-topic-per-word* probabilities from the model:

```{r}
#| code-line-numbers: "|2|3"
#| output-location: column-fragment
lda_tidy |>
  tidy(matrix = "beta") |>
  arrange(desc(beta))
```


## Interpreting Topics

To understand the model clearly, we need to see what terms are in each topic.

. . .

Starting with two topics:

```{r}
#| code-line-numbers: "|1,2,3,4,5|7|9|10|11|12|13"
lda_2_topics <- dtm_review |>
  LDA(
    k = 2,
    method = "Gibbs",
    control = list(seed = 123)
    ) |>
  tidy(matrix = "beta")

word_2_probs <- lda_2_topics |>
  group_by(topic) |>
  slice_max(beta, n = 15) |>
  ungroup() |>
  mutate(term2 = fct_reorder(term, beta))
```


## Interpreting Topics - Two Topics

```{r}
#| code-line-numbers: "|2,3,4,5,6,7,8|9|10|11|12,13,14"
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
word_2_probs |>
  ggplot(
    aes(
      x = term2,
      y = beta,
      fill = as.factor(topic)
      )
  ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip() +
  labs(
    title = "LDA Top Terms for 2 Topics"
  )
```


## Finding the Terms Generating the Greatest Differences

```{r}
#| code-line-numbers: "|2|3|4|5|8|9|10"
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
beta_wide <- lda_2_topics |>
  mutate(topic = paste0("topic", topic)) |>
  pivot_wider(names_from = topic, values_from = beta) |>
  filter(topic1 > .001 | topic2 > .001) |>
  mutate(log_ratio = log2(topic2 / topic1))

beta_wide |>
  arrange(desc(abs(log_ratio))) |>
  head(20) |>
  arrange(desc(log_ratio)) |>
  ggplot(
    aes(
      x = log_ratio,
      y = term
      )
    ) +
  geom_col(show.legend = FALSE) +
  labs(
    title = "Terms with the greatest difference in beta between two topics"
  )
```


## Interpreting Topics - Three Topics

```{r}
lda_3_topics <- dtm_review |>
  LDA(
    k = 3,
    method = "Gibbs",
    control = list(seed = 123)
    ) |>
  tidy(matrix = "beta")

word_3_probs <- lda_3_topics |>
  group_by(topic) |>
  slice_max(beta, n = 15) |>
  ungroup() |>
  mutate(term2 = fct_reorder(term, beta))
```


## Interpreting Topics - Three Topics

```{r}
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
word_3_probs |>
  ggplot(
    aes(
      x = term2, 
      y = beta, 
      fill = as.factor(topic)
      )
  ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip() +
  labs(
    title = "LDA Top Terms for 3 Topics"
  )
```


## Interpreting Topics - Four Topics

```{r}
lda_4_topics <- LDA(
  dtm_review,
  k = 4,
  method = "Gibbs",
  control = list(seed = 123)
) |>
  tidy(matrix = "beta")

word_4_probs <- lda_4_topics |>
  group_by(topic) |>
  slice_max(beta, n = 15) |>
  ungroup() |>
  mutate(term2 = fct_reorder(term, beta))
```


## Interpreting Topics - Four Topics

```{r}
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
word_4_probs |>
  ggplot(
    aes(
      x = term2,
      y = beta,
      fill = as.factor(topic)
      )
    ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  labs(
    title = "LDA Top Terms for 4 Topics"
  )
```


## Interpreting Topics - Five Topics

```{r}
lda_5_topics <- LDA(
  dtm_review,
  k = 5,
  method = "Gibbs",
  control = list(seed = 123)
) |>
  tidy(matrix = "beta")

word_5_probs <- lda_5_topics |>
  group_by(topic) |>
  slice_max(beta, n = 15) |>
  ungroup() |>
  mutate(term2 = fct_reorder(term, beta))
```


## Interpreting Topics - Five Topics

```{r}
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
word_5_probs |>
ggplot(
  aes(
    x = term2,
    y = beta,
    fill = as.factor(topic)
    )
  ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  labs(
    title = "LDA Top Terms for 5 Topics"
  )
```


## The Art of Topic Selection

-   Unsupervised learning always requires human interpretation...
-   Including new topics which are different is always a good thing.
-   When topics start repeating, we have selected too many!
-   Topics can be named based on the combination of high-probability words.


## The Art of Topic Selection

Don't forget about the mixed-membership concept and that these topics are not meant to be completely disjoint.

. . .

You can fine tune the LDA algorithm using extra parameters to improve the model.


## Document-Topic Probabilities

LDA models each document as a mix of topics and words.

. . .

With `matrix = "gamma"` we can investigate *per-document-per-topic* probabilities.


## Document-Topic Probabilities

Each of these values represents an estimated percentage of the document's words that are from each topic. 

. . .

Most of these reviews belong to more than one topic:

```{r}
#| code-line-numbers: "|2|3"
#| output-location: fragment
lda_tidy |>
  tidy(matrix = "gamma") |>
  arrange(desc(gamma))
```


# Colophon - Summary


## *Part One*: Text Mining and Exploratory Analysis

- We covered several topics:

  - `Tidy` and `|>`
  - Process of Text Mining
  - Data Exploration
  - Text Cleaning with `textclean`
  - Tokenizing Text with `tidytext`
  - Removal of stop words
  - Plotting Words (Counts and Clouds)
  
  
## *Part Two*: Tidy Sentiment Analysis in R

- We explored:

  - Sentiment Analysis Dictionaries
  - Counting and Visualizing Sentiments
  - Comparing Sentiments (Counts and Clouds)


## *Part Three*: Topic Modelling

- We discussed:

  - Clustering vs Topic Modelling
  - Topic Modelling with LDA
  - Create the Document-Term Matrix
  - Interpreting Topics
  - Finding the Terms Generating Differences
  - Document-Topic Probabilities
  - The Art of Topic Selection


## Outro {auto-animate=true}

::: {style="font-size: 4em; text-align: center"}

Thank you! :raised_hands:

:::

::: {style="font-size: 1.2em; text-align: center"}

Feel free to explore the Appendix!

:::


# References

::: {#refs}

:::


# Appendix


## Sentiment Analysis with `sentimentr`

- Another package for lexicon-based sentiment analysis is [`sentimentr`](https://github.com/trinker/sentimentr) [@sentimentr]. 

- Unlike the [`tidytext`](https://juliasilge.github.io/tidytext/index.html) package, [`sentimentr`](https://github.com/trinker/sentimentr) takes valence shifters (e.g., negation) into account, which can easily flip the polarity of a sentence with one word.


## Sentiment Analysis with `sentimentr`

- For example, the sentence *“I am not unhappy”* is actually positive.
- But if we analyze it word by word, the sentence may seem to have a negative sentiment due to the words *“not”* and *“unhappy”*. 
- Similarly, *“I hardly like this book”* is a negative sentence.
- But the analysis of individual words, *“hardly”* and *“like”*, may yield a positive sentiment score.


## Sentiment Analysis with `sentimentr`

In contrast to [`tidytext`](https://juliasilge.github.io/tidytext/index.html), for [`sentimentr`](https://github.com/trinker/sentimentr) we need the actual sentences rather than the individual tokens.

. . .

Therefore, we can:

- use the original cleaned `review_data`, 
- get individual sentences for each media briefing using the [`get_sentences()`](https://github.com/trinker/sentimentr?tab=readme-ov-file#functions) function, and 
- calculate sentiment scores per sentence via `sentiment()`.

. . .

As usual, we need to install the package:

````r
install.package(sentimentr)
````

. . . 

And load it before usage:

```{r}
library(sentimentr)
```


## Sentiment Analysis with `sentimentr` {auto-animate=true}

```{r}
#| code-line-numbers: "|2|3|4"
sentimentr_review <- review_data |>
  mutate(id = row_number()) |>
  get_sentences() |>
  sentiment()
```


## Sentiment Analysis with `sentimentr` {auto-animate=true}

```{r}
#| output-location: fragment
sentimentr_review <- review_data |>
  mutate(id = row_number()) |>
  get_sentences() |>
  sentiment()

sentimentr_review
```


## Sentiment Analysis with `sentimentr` - Plotting by Star Rating

```{r}
#| code-line-numbers: "|2|3-9|10|11|12-17"
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
sentimentr_review |>
  group_by(stars) |>
  ggplot(
    aes(
      x = stars,
      y = sentiment,
      fill = as.factor(stars)
      )
    ) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Overall Sentiment by Stars using sentimentr",
    subtitle = "Reviews for Alexa",
    x = "Stars",
    y = "Overall Sentiment"
  )
```


## Sentiment Analysis with `sentimentr`

We can also look at sentiment analysis by whole reviews, instead of per sentence.

```{r}
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
sentimentr_sentence <- review_data |>
  get_sentences() |>
  sentiment_by() 

review_data_id <- review_data |>
    mutate(id = row_number()) 

sentimentr_merged <- sentimentr_sentence |>
  inner_join(review_data_id,
             join_by(element_id == id))

sentimentr_merged |>
  group_by(stars) |>
  ggplot(
    aes(
      x = stars, 
      y = ave_sentiment,
      fill = as.factor(stars)
      )
    ) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Overall Sentiment by Stars using sentimentr",
    subtitle = "Reviews for Alexa",
    x = "Stars",
    y = "Overall Sentiment"
  )
```


## Sentiment Analysis with `sentimentr`

For this specific case, we can see that the sentiment analysis results are very similar between:

:::: {.columns}

::: {.column width="50%"}

::: {style="font-size: 0.5em; text-align: center"}

`sentimentr` at a sentence level

:::

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 7
sentimentr_merged |>
  group_by(stars) |>
  ggplot(
    aes(
      x = stars, 
      y = ave_sentiment,
      fill = as.factor(stars)
      )
    ) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Overall Sentiment by Stars using sentimentr",
    subtitle = "Reviews for Alexa",
    x = "Stars",
    y = "Overall Sentiment"
  )
```
:::

::: {.column width="50%"}

::: {style="font-size: 0.5em; text-align: center"}

`bing` lexicon on a word by word basis

:::

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 7
tidy_review |>
  inner_join(get_sentiments("bing")) |>
  count(stars, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n) |>
  mutate(
    overall_sentiment = positive - negative,
    stars2 = reorder(stars, overall_sentiment)
         ) |>
  ggplot(
       aes(
         x = stars2, 
         y = overall_sentiment, 
         fill = as.factor(stars)
         )
       ) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Overall Sentiment by Stars (bing lexicon)",
    subtitle = "Reviews for Alexa",
    x = "Stars",
    y = "Overall Sentiment"
  )
```

:::

::::


## (Alternative) Creating Word Clouds

Unfortunately, there is no `tidy` way to create a word cloud with the `wordcloud` package :unamused:

Regardless, it is time to ask the [`wordcloud()`](https://www.rdocumentation.org/packages/wordcloud/versions/2.6/topics/wordcloud) function to read and plot our data:

```{r}
#| code-line-numbers: "|1|2|3|"
#| output-location: slide
#| fig-width: 15
#| fig-height: 7
wordcloud(
  word = word_counts$word,
  freq = word_counts$n
)
```


## (Alternative) Creating Word Clouds

There are some useful arguments to experiment with here:

-   [`min.freq`](https://www.rdocumentation.org/packages/wordcloud/versions/2.6/topics/wordcloud) and [`max.words`](https://www.rdocumentation.org/packages/wordcloud/versions/2.6/topics/wordcloud) set boundaries for how populated the wordcloud will be
-   [`random.order`](https://www.rdocumentation.org/packages/wordcloud/versions/2.6/topics/wordcloud) will put the largest word in the middle if set to FALSE
-   [`rot.per`](https://www.rdocumentation.org/packages/wordcloud/versions/2.6/topics/wordcloud) is the fraction of words that will be rotated in the graphic

. . .

Finally, the words are arranged randomly somehow, and so for a repeatable graphic we need to specify a seed value with `set.seed()`.


## (Alternative) Creating Word Clouds

```{r}
#| code-line-numbers: "|1|2|3|4|5|6|7|8"
#| output-location: column-fragment
#| fig-width: 5
#| fig-height: 5
set.seed(13)
wordcloud(
  word = word_counts$word,
  freq = word_counts$n,
  max.words = 30,
  min.freq = 4,
  random.order = FALSE,
  rot.per = 0.25
)
```


## (Alternative) Creating Word Clouds

As explained, we can also change the number of words displayed in the cloud:

```{r}
#| code-line-numbers: "|5"
#| output-location: column-fragment
#| fig-width: 5
#| fig-height: 5
set.seed(13)
wordcloud(
  word = word_counts$word,
  freq = word_counts$n,
  max.words = 70,
  min.freq = 4,
  random.order = FALSE,
  rot.per = 0.25
)
```


## (Alternative) Creating Word Clouds

Using pre-defined colours:

```{r}
#| code-line-numbers: "|9"
#| output-location: column-fragment
#| fig-width: 5
#| fig-height: 5
set.seed(13)
wordcloud(
  word = word_counts$word,
  freq = word_counts$n,
  max.words = 30,
  min.freq = 4,
  random.order = FALSE,
  rot.per = 0.25,
  colors = "blue"
)
```


## (Alternative) Creating Word Clouds

Using funky colours, thanks to the [`RColorBrewer`](https://www.rdocumentation.org/packages/RColorBrewer/versions/1.1-3/topics/RColorBrewer) package and its large selection of [colour palettes](https://renenyffenegger.ch/notes/development/languages/R/packages/RColorBrewer/index):

```{r}
#| code-line-numbers: "|9"
#| output-location: column-fragment
#| fig-width: 5
#| fig-height: 5
set.seed(13)
wordcloud(
  word = word_counts$word,
  freq = word_counts$n,
  max.words = 30,
  min.freq = 4,
  random.order = FALSE,
  rot.per = 0.25,
  colors = brewer.pal(8, "Paired")
)
```


## (Alternative) Creating Word Clouds

If you need more customization (including non-latin characters), you can use the [`wordcloud2()`](https://github.com/Lchiffon/wordcloud2) function from the [`wordcloud2`](https://github.com/Lchiffon/wordcloud2) package^[the `wordcloud2()` visualization may not display correctly, so I encourage you to run the code!]:

```{r}
#| output-location: slide
library(wordcloud2)
set.seed(13)
wordcloud2(word_counts_filter,
           size = 2, 
           minRotation = -pi/2, 
           maxRotation = -pi/2)
```
